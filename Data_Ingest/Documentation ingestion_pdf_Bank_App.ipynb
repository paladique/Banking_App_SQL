{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Links to Github:\n",
    "gh repo clone MicrosoftDocs/SupportArticles-docs : Get Support Troubleshooting\n",
    "gh repo clone MicrosoftDocs/sql-docs : Get SQL Docs\n",
    "gh repo clone MicrosoftDocs/azure-databases-docs: Get Azure Databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install langchain_community langchain_sqlserver langchain_openai unstructured python-dotenv sqlalchemy pandas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# 1. Imports & Environment\n",
    "# -------------------------\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# For PDF partitioning/extraction\n",
    "from unstructured.partition.auto import partition\n",
    "from unstructured.chunking.title import chunk_by_title\n",
    "\n",
    "# Progress\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# For embeddings\n",
    "import openai\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "\n",
    "# For SQL\n",
    "import urllib.parse\n",
    "import pyodbc\n",
    "from sqlalchemy import create_engine, text\n",
    "from urllib.parse import quote_plus\n",
    "\n",
    "# For vector store\n",
    "from langchain_sqlserver import SQLServer_VectorStore\n",
    "from langchain_community.vectorstores.utils import DistanceStrategy\n",
    "from langchain_openai import AzureChatOpenAI, AzureOpenAIEmbeddings\n",
    "import urllib.parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the .env file from the specified path\n",
    "load_dotenv(override=True)\n",
    "\n",
    "AZURE_OPENAI_KEY = os.getenv('AZURE_OPENAI_KEY')\n",
    "AZURE_OPENAI_ENDPOINT = os.getenv('AZURE_OPENAI_ENDPOINT')\n",
    "AZURE_OPENAI_DEPLOYMENT_EMBED = os.getenv('AZURE_OPENAI_EMBEDDING_DEPLOYMENT')\n",
    "AZURE_OPENAI_DEPLOYMENT = os.getenv('AZURE_OPENAI_DEPLOYMENT')\n",
    "\n",
    "# SQL DB credentials\n",
    "# --- Database Configuration ---\n",
    "server = os.getenv('DB_SERVER')\n",
    "database = os.getenv('DB_DATABASE')\n",
    "driver = os.getenv('DB_DRIVER', 'ODBC Driver 18 for SQL Server')\n",
    "client_id = os.getenv('AZURE_CLIENT_ID')\n",
    "client_secret = os.getenv('AZURE_CLIENT_SECRET')\n",
    "\n",
    "connection_string = (\n",
    "    f\"DRIVER={driver};\"\n",
    "    f\"SERVER={server};\"\n",
    "    f\"DATABASE={database};\"\n",
    "    \"AUTHENTICATION=ActiveDirectoryInteractive;\" # <-- Changed authentication method\n",
    "    \"Encrypt=yes;\"\n",
    "    \"TrustServerCertificate=no;\"\n",
    ")\n",
    "\n",
    "connection_url = f\"mssql+pyodbc:///?odbc_connect={quote_plus(connection_string)}\"\n",
    "\n",
    "engine = create_engine(connection_url, connect_args={\"connect_timeout\": 30})\n",
    "\n",
    "\n",
    "# Setup Vector Store\n",
    "# --- Instantiate your AzureOpenAIEmbeddings ---\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    azure_deployment=os.getenv(\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT\"),\n",
    "    openai_api_version=\"2024-10-21\",\n",
    "    openai_api_key=os.getenv(\"AZURE_OPENAI_KEY\"),\n",
    ")\n",
    "\n",
    "# --- Build the VectorStore object ---\n",
    "vector_store = SQLServer_VectorStore(\n",
    "    connection_string=connection_url,           # same ODBC DSN used above\n",
    "    distance_strategy=DistanceStrategy.COSINE,  # or DOT_PRODUCT, etc.\n",
    "    embedding_function=embeddings,              # text-embedding-ada-002\n",
    "    embedding_length=1536,                      # Vector dimension\n",
    "    table_name=\"DocsChunks_Embeddings\",         # Use the name you prefer\n",
    ")\n",
    "\n",
    "# Use AzureChatOpenAI for chat completions\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "    azure_deployment=AZURE_OPENAI_DEPLOYMENT,\n",
    "    openai_api_version=\"2024-10-21\",\n",
    "    openai_api_key=AZURE_OPENAI_KEY,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------\n",
    "# 2. PDF Parsing\n",
    "# ---------------\n",
    "def chunk_text(text: str, chunk_size: int = 500, overlap: int = 100) -> list:\n",
    "    \"\"\"\n",
    "    Simple text-chunking utility.\n",
    "    Splits the text into chunks of `chunk_size` characters\n",
    "    with `overlap` characters overlap between chunks.\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        chunk = text[start:end]\n",
    "        chunks.append(chunk)\n",
    "        start = end - overlap  # move the start back by overlap\n",
    "        if start < 0:\n",
    "            start = 0\n",
    "    return chunks\n",
    "\n",
    "pdf_path = \"RAG_Preparation/SecureBank - Frequently Asked Questions.pdf\"\n",
    "\n",
    "elements = partition(pdf_path)\n",
    "pdf_text = \"\\n\".join([el.text for el in elements if el.text])  # combine into one big string\n",
    "\n",
    "# Optional: remove blank lines or do other cleanup\n",
    "pdf_text = pdf_text.strip()\n",
    "\n",
    "# Break into smaller chunks for embeddings\n",
    "chunks = chunk_text(pdf_text, chunk_size=500, overlap=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 3. Write raw chunks to SQL\n",
    "# ----------------------------\n",
    "# Suppose we create a table [PDF_RawChunks] with columns:\n",
    "#   id INT IDENTITY(1,1) PRIMARY KEY\n",
    "#   chunk_text NVARCHAR(MAX)\n",
    "#   source_pdf NVARCHAR(512)  (optional, if you want to store PDF name/path)\n",
    "#   created_at DATETIME2 DEFAULT GETDATE() (optional)\n",
    "#\n",
    "# Adjust as necessary if your table already exists.\n",
    "\n",
    "create_table_sql = \"\"\"\n",
    "IF NOT EXISTS (SELECT * FROM sys.objects WHERE object_id = OBJECT_ID(N'[dbo].[PDF_RawChunks]') AND type in (N'U'))\n",
    "BEGIN\n",
    "    CREATE TABLE [dbo].[PDF_RawChunks] (\n",
    "        [id] INT IDENTITY(1,1) NOT NULL PRIMARY KEY,\n",
    "        [chunk_text] NVARCHAR(MAX) NOT NULL,\n",
    "        [source_pdf] NVARCHAR(512) NULL,\n",
    "        [created_at] DATETIME2 NOT NULL DEFAULT GETDATE()\n",
    "    );\n",
    "END\n",
    "\"\"\"\n",
    "with engine.begin() as conn:\n",
    "    conn.execute(text(create_table_sql))\n",
    "\n",
    "\n",
    "test_chunks = chunks[:200]  # comment this out later to process ALL chunks\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    trans = conn.begin()\n",
    "    for i, ck in enumerate(\n",
    "        tqdm(test_chunks, desc=\"Inserting in 3x100 test batches\", unit=\"chunk\"),\n",
    "        start=1\n",
    "    ):\n",
    "        conn.execute(\n",
    "            text(\"INSERT INTO [dbo].[PDF_RawChunks] (chunk_text, source_pdf) VALUES (:ctext, :spdf)\"),\n",
    "            {\"ctext\": ck, \"spdf\": pdf_path}\n",
    "        )\n",
    "\n",
    "        # Commit every 100 inserts\n",
    "        if i % batch_size == 0:\n",
    "            trans.commit()\n",
    "            # If there's more data left to insert, start a new transaction\n",
    "            if i < len(test_chunks):\n",
    "                trans = conn.begin()\n",
    "\n",
    "    # If the total number isn't an exact multiple of batch_size\n",
    "    # commit leftover rows in the final partial batch\n",
    "    if len(test_chunks) % batch_size != 0:\n",
    "        trans.commit()\n",
    "\n",
    "print(f\"Inserted {len(test_chunks)} chunks in batches of {batch_size}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Fetch Chunks from PDF_RawChunks That Need Embeddings\n",
    "select_sql = \"\"\"\n",
    "SELECT RC.id, RC.chunk_text, RC.source_pdf\n",
    "FROM PDF_RawChunks RC\n",
    "WHERE NOT EXISTS (\n",
    "    SELECT 1\n",
    "    FROM DocsChunks_Embeddings VEC\n",
    "    WHERE VEC.custom_id = CAST(RC.id AS VARCHAR(50))\n",
    ")\n",
    "ORDER BY RC.id\n",
    "\"\"\"\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    result = conn.execute(text(select_sql))\n",
    "    rows = result.fetchall()\n",
    "\n",
    "print(f\"Found {len(rows)} row(s) in PDF_RawChunks with no existing embedding.\")\n",
    "\n",
    "# 2) Insert in Batches via vector_store.add_texts()\n",
    "\n",
    "batch_size = 100  # commit in batches of 100\n",
    "\n",
    "# Convert each row into text + metadata\n",
    "all_texts = []\n",
    "all_metadata = []\n",
    "\n",
    "for row in rows:\n",
    "    row_id   = row[0]\n",
    "    text_val = row[1]\n",
    "    pdf_path = row[2]\n",
    "\n",
    "    # build your metadata\n",
    "    meta_dict = {\n",
    "        \"custom_id\": str(row_id),   # store the PDF_RawChunks ID as a string\n",
    "        \"source_pdf\": pdf_path\n",
    "    }\n",
    "\n",
    "    all_texts.append(text_val)\n",
    "    all_metadata.append(meta_dict)\n",
    "\n",
    "print(f\"Preparing to insert {len(all_texts)} texts in batches of {batch_size}...\")\n",
    "\n",
    "from math import ceil\n",
    "\n",
    "num_rows = len(all_texts)\n",
    "num_batches = ceil(num_rows / batch_size)\n",
    "\n",
    "index = 0\n",
    "for b in range(num_batches):\n",
    "    # Slice out a batch\n",
    "    batch_texts = all_texts[index : index+batch_size]\n",
    "    batch_meta  = all_metadata[index : index+batch_size]\n",
    "    index += batch_size\n",
    "\n",
    "    print(f\"Batch {b+1}/{num_batches}: inserting {len(batch_texts)} items...\")\n",
    "    \n",
    "    # The vector_store call *immediately* does embeddings + inserts\n",
    "    # into the underlying table. So each call is effectively a \"mini commit.\"\n",
    "    vector_store.add_texts(\n",
    "        texts=batch_texts,\n",
    "        metadatas=batch_meta\n",
    "    )\n",
    "\n",
    "print(\"All missing rows have been embedded and inserted into PDF_RawChunks_Embeddings!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What are the late payment fees on credit cards?\"\n",
    "docs = vector_store.similarity_search(query, k=3)\n",
    "for i, doc in enumerate(docs, 1):\n",
    "    print(f\"\\nResult {i}:\\nMetadata = {doc.metadata}\\nText    = {doc.page_content[:150]}...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
